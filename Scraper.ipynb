{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8f3421-77f7-4dc3-91c9-df20d9ccd969",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a7438-8ce0-47fc-b431-86dc2724a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm  \n",
    "from datetime import datetime\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9dd28f-44d1-4dc3-9e3a-7f701f562063",
   "metadata": {},
   "source": [
    "# FED Scraper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416e886-0571-4ee6-9855-8bec9451018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_1996_2005 = pd.DataFrame(columns=['link', 'title', 'event', 'year'])\n",
    "\n",
    "# Scraping para los años 1996-2005\n",
    "years = range(1996, 2006)\n",
    "for year in years:\n",
    "    speeches_one_year = pd.DataFrame()\n",
    "    page = requests.get(f'https://www.federalreserve.gov/newsevents/speech/{year}speech.htm')\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    title = soup.select(\".title\")\n",
    "    locations = soup.select(\".location\")\n",
    "    for i in range(len(title)):\n",
    "        speeches_one_year.at[i, 'link'] = 'https://www.federalreserve.gov' + title[i].find_all('a', href=True)[0]['href']\n",
    "        speeches_one_year.at[i, 'title'] = title[i].text.split('\\n')[1]\n",
    "        speeches_one_year.at[i, 'event'] = locations[i].text.split('\\n')[1].strip()\n",
    "        speeches_one_year.at[i, 'year'] = year\n",
    "    if not speeches_one_year.empty:\n",
    "        all_years_1996_2005 = pd.concat([all_years_1996_2005, speeches_one_year], ignore_index=True)\n",
    "\n",
    "# Para los discursos con años anteriores a 1999\n",
    "old_site_version_length = sum(all_years_1996_2005['year'] < 1999)\n",
    "for i in range(old_site_version_length):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    page = requests.get(all_years_1996_2005.loc[i, 'link'])\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    text_list = [i for i in soup.find('p').getText().split('\\n') if i]\n",
    "    text_list = text_list[:-8]  # Eliminar últimas líneas irrelevantes\n",
    "    text_list = '\\n'.join(text_list)\n",
    "    text_list = text_list.replace('--', ' ').replace('\\r', '').replace('\\t', '')\n",
    "    all_years_1996_2005.loc[i, 'text'] = text_list\n",
    "\n",
    "# Para los discursos entre 1999 y 2005\n",
    "for i in range(len(all_years_1996_2005)):\n",
    "    if 1998 < all_years_1996_2005.loc[i, 'year'] < 2006:\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "        page = requests.get(all_years_1996_2005['link'].iloc[i])\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        events = soup.select(\"table\")\n",
    "        text_list = events[0].text if len(str(events[0].text)) > 600 else events[1].text\n",
    "        text_list = ''.join(text_list).replace('--', '').replace('\\r', '\\n').replace('\\t', '')\n",
    "        if 383 <= i <= 536:\n",
    "            text_list = text_list.replace('     ', ' ').replace('    ', ' ')\n",
    "        all_years_1996_2005.loc[i, 'text'] = text_list\n",
    "\n",
    "all_years_1996_2005['date'] = all_years_1996_2005['link'].str.extract('(\\d\\d\\d\\d\\d\\d\\d\\d)')\n",
    "all_years_1996_2005 = all_years_1996_2005[~all_years_1996_2005['text'].isna()]\n",
    "all_years_1996_2005['text_len'] = all_years_1996_2005['text'].str.split().apply(len)\n",
    "all_years_1996_2005['location'] = all_years_1996_2005.event.str.split(', ').apply(lambda x: x[-1])\n",
    "\n",
    "all_years_1996_2005.to_csv('fed_speeches_1996_2005.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064559c2-5e85-4c1b-b1f2-ea1dc784c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.federalreserve.gov\"\n",
    "speeches_url = f\"{base_url}/newsevents/speeches.htm\"\n",
    "response = requests.get(speeches_url)\n",
    "response.raise_for_status()  # solicitud exitosa\n",
    "\n",
    "# Parsear\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "speech_links = soup.select(\"a[href^='/newsevents/speech/']\")\n",
    "speech_urls = [f\"{base_url}{link['href']}\" for link in speech_links]\n",
    "\n",
    "# Guardar las URLs en un archivo CSV\n",
    "with open(\"fed_speech_urls.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"URL\"])  # Encabezado del CSV\n",
    "    for url in speech_urls:\n",
    "        writer.writerow([url])\n",
    "print(f\"Se han guardado {len(speech_urls)} URLs de discursos en 'fed_speech_urls.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76112dea-3023-49b8-8c25-112defbfbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_urls = []\n",
    "with open(\"fed_speech_urls.csv\", mode=\"r\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        annual_urls.append(row[\"URL\"])\n",
    "\n",
    "# URL base para construir las URLs completas de discursos individuales\n",
    "base_url = \"https://www.federalreserve.gov\"\n",
    "all_speech_urls = []\n",
    "\n",
    "# Para cada URL de discursos anuales\n",
    "for annual_url in annual_urls:\n",
    "    # Hacer la solicitud a la página anual\n",
    "    response = requests.get(annual_url)\n",
    "    response.raise_for_status()  # solicitud exitosa\n",
    "\n",
    "    # Parsear\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    speech_links = soup.select(\"a[href^='/newsevents/speech/']\")\n",
    "    for link in speech_links:\n",
    "        url = f\"{base_url}{link['href']}\"\n",
    "        # Filtrar URLs para evitar duplicados o enlaces que no sean discursos específicos\n",
    "        if url not in all_speech_urls and url.endswith(\".htm\"):\n",
    "            all_speech_urls.append(url)\n",
    "\n",
    "# Guardar las URLs en un archivo CSV\n",
    "with open(\"all_fed_speech_urls.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"URL\"])  \n",
    "    for url in all_speech_urls:\n",
    "        writer.writerow([url])\n",
    "print(f\"Se han guardado {len(all_speech_urls)} URLs de discursos en 'all_fed_speech_urls.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a5735-4c2f-4c7d-8305-a1c36ac7f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_urls = []\n",
    "with open(\"all_fed_speech_urls.csv\", mode=\"r\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        speech_urls.append(row[\"URL\"])\n",
    "speeches_data = []\n",
    "\n",
    "# Patrón regex\n",
    "date_pattern = re.compile(r\"\\b(\\d{1,2}/\\d{1,2}/\\d{4})\\b\")\n",
    "for url in tqdm(speech_urls, desc=\"Procesando discursos\", unit=\"discurso\"):\n",
    "    # Hacer la solicitud a la página del discurso\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # solicitud exitosa\n",
    "\n",
    "    # Parsear\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    date_text = \"\"\n",
    "    match = date_pattern.search(soup.get_text())\n",
    "    if match:\n",
    "        date_text = match.group(0)  # Capturamos la fecha encontrada\n",
    "    content_div = soup.find(\"div\", {\"class\": \"col-xs-12 col-sm-8 col-md-8\"})\n",
    "    speech_content = content_div.get_text(separator=\" \", strip=True) if content_div else \"\"\n",
    "    speeches_data.append({\n",
    "        \"Fecha\": date_text,\n",
    "        \"URL\": url,\n",
    "        \"Discurso\": speech_content\n",
    "    })\n",
    "\n",
    "# Guardar los datos en un archivo CSV\n",
    "with open(\"fed_speeches_2006_Today.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Fecha\", \"URL\", \"Discurso\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(speeches_data)\n",
    "print(\"Todos los discursos han sido extraídos y guardados en 'fed_speeches_2006_Today.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026ffe4-d9ff-4a0b-a0e5-fe562300dce6",
   "metadata": {},
   "source": [
    "# Big Ass Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1ef9f-b93d-4da0-b840-3a99faa264d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"fed_speeches_2006_Today.csv\"\n",
    "output_file = \"fed_speeches_2006_Today_with_dates.csv\"\n",
    "date_pattern = re.compile(r\"(\\d{8})a\\.htm\")\n",
    "updated_data = []\n",
    "with open(input_file, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # Extraer la fecha de la URL\n",
    "        match = date_pattern.search(row[\"URL\"])\n",
    "        if match:\n",
    "            # Convertir AAAAMMDD a AAAA-MM-DD\n",
    "            date_str = match.group(1)\n",
    "            date_formatted = datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "            # Guardar la fecha formateada en la columna \"Fecha\"\n",
    "            row[\"Fecha\"] = date_formatted\n",
    "        else:\n",
    "            row[\"Fecha\"] = \"\"\n",
    "        updated_data.append(row)\n",
    "\n",
    "# Archivo CSV con la columna Fecha actualizada\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Fecha\", \"URL\", \"Discurso\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(updated_data)\n",
    "print(f\"El archivo con las fechas actualizadas se ha guardado como '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f83dad-912f-4fd5-bff6-f7188ad52628",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv1 = pd.read_csv(\"fed_speeches_1996_2005.csv\", usecols=[\"date\", \"link\", \"text\"])\n",
    "# Convertir la columna de fecha al formato AAAA-MM-DD\n",
    "csv1[\"date\"] = pd.to_datetime(csv1[\"date\"], errors='coerce').dt.strftime(\"%Y-%m-%d\")\n",
    "# Renombrar columnas para que coincidan con el formato final\n",
    "csv1 = csv1.rename(columns={\n",
    "    \"date\": \"Date\",\n",
    "    \"link\": \"URL\",\n",
    "    \"text\": \"Speech\"\n",
    "})\n",
    "# Leer el segundo CSV (2006 a hoy)\n",
    "csv2 = pd.read_csv(\"fed_speeches_2006_Today_with_dates.csv\", usecols=[\"Fecha\", \"URL\", \"Discurso\"])\n",
    "csv2[\"Fecha\"] = pd.to_datetime(csv2[\"Fecha\"], errors='coerce').dt.strftime(\"%Y-%m-%d\")\n",
    "csv2 = csv2.rename(columns={\n",
    "    \"Fecha\": \"Date\",\n",
    "    \"URL\": \"URL\",\n",
    "    \"Discurso\": \"Speech\"\n",
    "})\n",
    "combined_data = pd.concat([csv1, csv2], ignore_index=True)\n",
    "combined_data.to_csv(\"Data_1996_Today_FED.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"El archivo combinado se ha guardado como 'Data_1996_Today_FED.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97baf5d1-214c-4374-bdc7-05ecfd0cd3aa",
   "metadata": {},
   "source": [
    "# Limpiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9d783-c1a9-4a22-ac4a-b73b444308b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de archivos a eliminar\n",
    "files_to_delete = [\n",
    "    \"all_fed_speech_urls.csv\",\n",
    "    \"fed_speech_urls.csv\",\n",
    "    \"fed_speeches_1996_2005.csv\",\n",
    "    \"fed_speeches_2006_Today_with_dates.csv\",\n",
    "    \"fed_speeches_2006_Today.csv\"\n",
    "]\n",
    "for file in files_to_delete:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(f\"{file} ha sido eliminado exitosamente.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file} no se encontró o ya fue eliminado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo eliminar {file}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
